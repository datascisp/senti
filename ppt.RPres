```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
<style>
.small-code pre code {
  font-size: 1em;
}


ppt
========================================================
author: Vishal Shukla
date: 
autosize: true


========================================================
class: small-code
Save data as csv as it is way faster than xlsx. 

- Check which records dont have pollarity assigned and take care of it.
```{r, include=FALSE}
require(tm)
require(caTools)
twt <- read.csv("twitter.csv")
twt[9422, ]$polarity  <- -1
twt[10269, ]$polarity  <- 1
split <- sample.split(twt$text, SplitRatio = 0.75)
train <- subset(twt, split == T)
test <- subset(twt, split == F)

toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

trainvector <- as.vector(train$text)
corp <- Corpus(VectorSource(trainvector))

corp <- tm_map(corp, toSpace, "@")
corp <- tm_map(corp, toSpace, "#")
corp <- tm_map(corp, toSpace, "$")
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, stemDocument, language = "english")
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, stripWhitespace)
traintermmatrix <- DocumentTermMatrix(corp, control = list(weighting = weightTf, stopwords = T))
```

```{r eval=F}
which(is.na(twt$polarity))
```

- Check out class distribution
```{r eval=T}
table(twt$polarity)
```

Doc Term Matrix
========================================================
class: small-code
```{r, echo=TRUE, message=FALSE, warning=FALSE}
traintermmatrix
```
We cut down the matrix drastically by removing sparse terms.
```{r}
traintermmatrix <- removeSparseTerms(traintermmatrix, 0.99)
traintermmatrix
```
From 12930 terms to 163 terms

Dictionary reduction methods
========================================================
- Use local dictionary (becomes bag of words model)
- Remove stopwords
- Feature selection
- Token reduction (by stemming, lemmatization, merging synomyms) 

Class imbalance problem
=======================================================
class: small-code
More than 60% of tweets are negative causing significant class imbalance which can have adverse effect on modelling.

The issue can be addressed in the following ways:
- Collect more data (we should not go for it at this stage)
- Upsample / Downsample the imbalanced classes
- Cost sensitive training
  + can specify costs for missclassified minority classes as shown below
  
```{r, eval=F}
# In case of C5.0
costMatrix <- matrix(c(0,5,5,10,0,7,6,5,0), nrow = 3)
c50 <- C5.0(as.matrix(traintermmatrix), as.factor(train$polarity), costs = costMatrix)
```

Bigrams
========================================================
class: small-code
```{r}
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
BigramMatrix <- TermDocumentMatrix(corp, control = list(tokenize = BigramTokenizer))
```
Top five bigram pairs
```{r, echo=F}
BigramMatrix <- removeSparseTerms(BigramMatrix, 0.98)
freqBi <- rowSums(as.matrix(BigramMatrix))
freqBi <- sort(freqBi, decreasing = TRUE)
freqBi <- as.data.frame(freqBi)
freqBi <- data.frame(rownames(freqBi), freqBi$freqBi)
colnames(freqBi) <- c("word pairs", "frequency")
x <- subset(freqBi[1:30,])
head(x, 5)
```

Correlation Plot
========================================================
class: small-code
We don't find any major patch of a color shade indicating correlation
```{r, echo=TRUE, out.width="380px", out.height="380px", fig.align = "center"}
library(corrplot)
c <- cor(as.matrix(traintermmatrix))
corrplot(c, method = "color")
```

==========================================================
class: small-code
We can remove correlated terms from the train term matrix as below
```{r eval=F}
library(caret)
correlated = findCorrelation(c, cutoff=0.8) # Find term pairs with more than 80% correlation
termmat <- as.matrix(traintermmatrix)
termmat <- termmat[ , -c(correlated)] # Removing the correlated terms
```

LDA (Latent Dirichlet Allocation)
========================================================
class: small-code
Remove empty documents from the corpus first before applying LDA
```{r, eval=FALSE}
library(topicmodels)
lda <- LDA(traintermmatrix, k=10)
```

```{r echo=F}
rowTotals <- apply(as.matrix(traintermmatrix), 1, sum)
traintermmatrix.new   <- traintermmatrix[rowTotals> 0, ]
library(topicmodels)
lda <- LDA(traintermmatrix.new, k=10)
```

```{r}
terms(lda)
```
Can remove the handle names from corpus if not wanted as topics

Multi Dimensional Scaling (MDS) with LSA (Latent Symantic Analysis)
===================================================================
class: small-code
Words with close meanings occur in similar kind of documents. In this way LSA analyzes for similarity between the records.
```{r}
library(lsa)
traintermmatrix.lsa <- lw_bintf(as.matrix(traintermmatrix.new)) * gw_idf(as.matrix(traintermmatrix.new))  
lsaSpace <- lsa(traintermmatrix.lsa)  
dist.mat.lsa <- dist(t(as.textmatrix(lsaSpace)))
fit <- cmdscale(dist.mat.lsa, eig = TRUE, k = 3)
```

===================================================================
class: small-code
```{r fig.align = "center", out.height = "400px", out.width = "400px"}
library(scatterplot3d)
scatterplot3d(fit$points[, 1], fit$points[, 2], fit$points[, 3], pch = 16, main = "Semantic Space Scaled to 3D", xlab = "x", ylab = "y", zlab = "z", type = "p")
```

PCA (Principal Component Analysis)
==================================================
class: small-code
```{r fig.align = "center"}
x <- as.matrix(traintermmatrix.new)
F <- x/rowSums(x)
classpca <- prcomp(F, scale. = T)
biplot(classpca, scale = T)
```

SIMCA (Soft Independent Modelling of Class Analogies)
=====================================================
class: small-code
```{r eval = FALSE}
library(mdatools)
sim_neg <- simca(as.matrix(traintermmatrix), "-1")
sim_pos <- simca(as.matrix(traintermmatrix), "1")
sim_nut <- simca(as.matrix(traintermmatrix), "0")
simmulti <- simcam(list(sim_neg, sim_pos, sim_nut))

res_simca <- predict(simmulti, as.matrix(testtermmatrix))
```

SIMCA, a method for supervised classification uses PCA for retaining significant components from the data.


Using Clustering for Classification
==================================================
class: small-code

```{r fig.align = "center", out.height = "425px", out.width = "425px"}
library(cluster)
d <- dist(t(traintermmatrix), method="euclidian")
kfit <- kmeans(d, 3)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=3, lines=0)
```

=================================================
This is a simple cluster formed by kmeans. 

There are other ways with which we can apply clustering to new-data / test set. (Mclust package, etc.)

Although when we know the class of a record, it is a classification problem not clustering. But we can apply clustering anyway and check out the performance.


## Scope for improving performance
===================================================
We can improve model performance by implementing one or many of the following:
- Improve quality of data (Already done by removing irrelevant tweets)
- Add more features to the original data
  + Can add columns for subjectivity/objectivity
  + Can go for varying degrees of sentiments (like -5 to +5 instead of -1 to +1). This can increase the number of classes and hence reduce class imbalance.
- Feature engineering 
- Algorithm parameter tuning
  
None of the above method can single-handedly give miraculous rise in performance. But combination of all best practices can surely benefit.
